# Data

There were some challenges when searching for a dataset. The initial dataset was a [car evaluation dataset](https://archive.ics.uci.edu/dataset/19/car+evaluation). The features were quite close to the real dataset in terms of being ordinal from a relative small (4-8) set of categories with normal(ish) distributions, exponential distributions or uniform distributions. However, the dataset was too small and model training took only a few seconds. Since this was so quick and did not take up a notable amount of resources, it wasn't useful, since we want to utilize metrics like resource consumption and pipeline time to make assessments and improve the greenness of the pipeline. The new dataset was found on Kaggle, and is [Auto insurance claims](https://www.kaggle.com/datasets/bharatnatrayn/auto-insurance-claim). The dataset has 58 features, including categorical and ordinal feature variables with relatively few categories. We only use these features, and feature selection can be seen in `testing/testing2.ipynb`. The dataset is already encoded, so it's hard to tell what the different features are, but that does not matter for our purposes. Since the dataset is quite big (500k rows, 59 columns), it is not included in the repo. Note that the dataset indicates a binary `target` variable to be used as the label. I don't actually predict this variable, but instead use `ps_ind_02_cat` which has 4 categories as the label. This is more analogous to the real problem, where any feature from a predefined subset of the features can be used as the label based on prediction accuracy.