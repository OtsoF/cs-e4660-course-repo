# Study log 3

The machine learning pipeline we have at work, which I described in study log 2, is currently using native kubernetes CronJobs and regular batch jobs for experimentation and re-runs of failed steps. Complete end-to-end control of the pipeline is lacking and there is no coordination of the phases, or the ability to do fast recovery from failures, e.g. checkpoints. In short, we are lacking proper orchestration and a workflow engine to centrally manage and automate e2e workflows. For this reason, I decided to look at orchestration from the perspective of automation management.

I briefly compared Kubeflow, Apache Airflow and Argo Workflows, ultimately deciding on Argo Workflows. There were two main reasons: workflows are a CRD (custom resource definition in Kubernetes) and the workflows are defined ‘as code’, declarative YAML just like any other kubernetes definitions. This allows them to fit in the familiar infra-as-code workflows of DevOps people who are responsible for maintaining them. On the other hand, Argo Workflows offers a central UI, which is easy to use as an ML engineer to trigger the pipeline and view both an overview of the entire pipeline and see individual stages, their success statuses and logs to resolve failures. 

Argo workflows allow defining fully automated e2e workflows with workflow templates. A workflow template is a set of sequential steps or a DAG for more complex workflows. The steps are run based on conditions such as the success or results of previous steps, event triggers or timed schedules. Of course, it also benefits from being Kubernetes native e.g. multiple executors, parallelism, autoscaling, volumes and native metrics eg. with prometheus. Since the ML pipeline in question is rather simple (data query->preprocessing->store in db->model training->store model to blob->serve with model api), the whole pipeline can be automated quite easily as sequential steps with each step being run if the previous step is successful. Success conditions can be configured based on exit codes of scripts, output artifacts (or lack thereof). This simple e2e workflow can be triggered from the Argo Workflows UI, or it can be scheduled to run e.g. nightly. Workflows allow parameterization, so running experimentation of select workflow steps for eg. a specific feature branch is possible to parameterize. 

In this system complexity will come from having more of these rather simple pipelines instead of more complex pipelines. For example, the database which stores model training data, needs to serve multiple pipelines and experimentation, so either it needs to autoscale or be manually scaled based on usage, which requires coordination between pipelines using the database. This complexity will cause simple pipelines to not be reliable and pipelines can’t exactly be considered automated if they require manual intervention frequently. Argo does support retries, resubmissions and suspensions & resume for manual fixing, but pipelines can also be designed to be more resilient on their own. For example using automated retries or workflow template level synchronization (eg. semaphores, mutexes or locks) to manage db access or similar challenges.
