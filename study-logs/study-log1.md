# Study log 1

In all honesty, the first lecture was maybe a little hard to grasp. The topic was rather high level, without specific examples to support learning. The main idea of focusing on the whole system perspective, always considering what effects a change may have on the whole system was clear enough. What wasn’t so clear was what this actually means in practice. It was stressed that both systems and workflows are complex, and solving challenges such as cost, service performance, scalability and SRE are **hard** problems to solve. It also very much depends on the system and goals which of these are more important and which are less so. With my background, since there weren’t really specific examples, right now I don't even know how to approach these challenges or what solutions may look like. An example from the lecture relating to metrics was that base metrics such as cpu and memory are trivial (and not directly relevant on this course), but metrics such as ‘data quality’ and controlling its effects on the whole system could be interesting. What exactly controlling ‘data quality’ or what that even exactly can be or how to measure it is not clear to me. Of course, it depends on the system, and there is no single example, but with no previous knowledge I have a hard time imagining examples. 

As a DevOps engineer looking at our first ML pipeline at work, I would think from the R3E the most important concept for us right now is Reliability. Working with as a DevOps engineer topics like SRE are crucial and most familiar to me, so there is likely some bias. To me reliability is also, in a sense, the most ‘basic out of the four. Robustness relates to the ability to handle errors such as bad data. Resilience deals with remaining functional “in the face of adversity”. Elasticity deals with scaling both up and down due to outside forces. All of these deal with challenges coming from the outside world, and are somewhat outside of our control and thus ‘hard’ by default. (Mostly) reliable services can be built with common best practices available on modern cloud platforms such as high availability and geo redundancy. On cloud, ‘true’ reliability is of course out of our hands, and we just have to rely on the cloud provider, though some guarantees can be received in the form of SLAs. One other reason I think reliability is most important is that it is most transparent to customers. A slowly or poorly working service may go unnoticed, but a non-functioning service will not. Even if noticed, a badly performing service is better than a non-functional service. At least I would think this is the case for our system, but it of course depends on the system and how it is used. Customers may also be more understanding of issues caused by external factors. For example system attacks (resilience), bad data caused by customers themselves (robustness) or scaling issues due to unaccounted increase of customers (elasticity). At least I as a customer would be more understanding of these than not being able to use a service at all due to it being down. On the lecture there were mentions of ‘hard’ problems also with reliability: “have good and enough data, clean data” and “build pipelines without degraded performance and accuracy”. Again I have a hard time imagining a complex enough system, where it wouldn’t be possible to create build pipelines without degraded performance or accuracy (I would think just give more resources if the accuracy or performance is not good enough otherwise). But again I don’t have experience with these kinds of complex systems.

Our machine learning problem at work is quite simple, but to me it seems that the solution is needlessly complex. For this reason I chose to read the paper “Hidden Technical Debt in Machine Learning Systems” Obviously, it is harder to maintain reliability the more complex a system becomes, so removing unnecessary complexity is also desirable from this perspective. The paper does a good job of listing sources of technical debt, which aren’t too dissimilar from technical debt in ‘traditional systems’. Something new with machine learning systems is data dependencies, which according to the paper can cost more than code dependencies, something I hadn’t even thought of.  The big challenge is to actually identify these sources of technical debt, (be in data, code or infra) in our system, which I did not yet have time to do. A thorough analysis would likely be very time consuming, but hopefully larger things like antipatterns would be more easy to detect. One useful highlight in the paper is how to prevent technical debt. For example thinking about the impact of a change on the whole system. As noted in the lecture as well, a small change in one part of the system may have large consequences for the system as a whole, for example degraded elasticity. 
