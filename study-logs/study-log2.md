# Study log 2

I decided to review our first ML pipeline at work and think about monitoring. I picked it because on Azure (where our ML pipeline runs) it is easy to just use Azure monitor solutions, but for one we don’t systematically do that, and further, it’s a general purpose monitoring solution and likely does not support all ML monitoring use cases.

The goal of the ML system is to enable auto filling invoices based on historical invoices. The pipeline begins by importing data from data sources to a blob storage, where some data preprocessing is performed and the data is imported to an SQL table ready for model training. Model training uses the training data from SQL and trains models into a blob storage. This blob storage is finally consumed by an api which serves predictions to required applications. 

Monitoring this system could be broken down into two areas, runtime monitoring, and pipeline monitoring. Runtime monitoring covers actual running programs, cpu, memory and logs. Pipeline monitoring covers monitoring success/failure of pipelines, their duration & trends over time. On Azure, the Azure Monitor platform supports collecting and storing standard metrics such as cpu, memory and logs, and it allows creating custom metrics for more niche things that don’t yet exist, for example pipeline duration etc. These can be used to create alerts, custom dashboards or just queried with Log Analytics. The Azure Monitor solution is good and exhaustive, as it ‘stores everything’. But the tricky part is identifying key metrics, generating custom metrics, and using the metrics correctly, eg. alerts for specific metrics on specific thresholds, dashboards for trends that would like to be viewed often. 

For the system in question there are the most crucial things to monitor (in my view) are: 

1. Availability of the model API: In my opinion most important, since it is the first thing customers will notice, as they don’t get their predictions. Stale predictions are not as bad as no predictions. This is relatively simple to do with liveness probes. More challenging is to monitor prediction correctness, maybe with some controlled predictions, to see if they are predicted as expected. Likely the root cause of bad predictions is elsewhere so monitoring and detection issues should be at the root. Still good to monitor prediction quality at the model api, in case other monitors don’t catch issues.
2. Pipeline (data import, pre-processing, model training) failures: alert if they fail, and provide links to log analytics queries containing relevant info to determine failure (logs, duration, cpu, memory). I think the base version of this is easy to implement, but base metrics may not often be enough to determine root cause of failure, eg. model training fails, model does not meet requirements, why? Was the data bad? etc.  Need more metrics to link pipelines, eg, what data was used in this training, was it bad? Was there another issue?
3. Pipeline duration: Given pipelines run on Kubernetes with and are not (mainly) restricted by resources, duration of pipelines or the duration trend is useful to track. If the duration trend starts to rise, something is likely wrong, but again we need to monitor and retain metrics data, but also context data (what model was used, what data was used, what else was going on in the system at the time) to be able to determine the root cause. 

After having thought about what to monitor, I sort of realize how tricky it is. In every example I thought of, the base monitoring for failure is easy, but it only tells that there is an issue and more monitoring is needed to be able to determine root causes, or in the best case notice issues based on monitoring and preventatively fix things before there are ‘actual’ failures. 

